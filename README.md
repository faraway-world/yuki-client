# yuki-llama-client
A minimal local-first Python client for llama.cpp that provides persistent chat memory and real-time token streaming over a remote inference server.
